import dlt
import pandas as pd
from sqlalchemy import create_engine
from dlt.sources.helpers import requests

########## RESOURCES ##########

# Set a pipeline to load each resource individually
resource_pipeline = dlt.pipeline(
    pipeline_name="individual_resource_pipeline",
    destination="duckdb",
    dataset_name="individual_resource_schema",
)

# Sample pokemon data
pokemon_dict = [
    {"id": "1", "name": "bulbasaur", "size": {"weight": 6.9, "height": 0.7}},
    {"id": "4", "name": "charmander", "size": {"weight": 8.5, "height": 0.6}},
    {"id": "25", "name": "pikachu", "size": {"weight": 6, "height": 0.4}},
]

# A better way is to wrap it in the @dlt.resource decorator which denotes a logical
# grouping of data within a data source, typically holding data of similar structure and origin:
@dlt.resource(table_name="pokemon_dict_data", write_disposition="replace")
def pokemon_dict_resource():
    '''
    Common arguments in a resource:
    - name: The resource name and the name of the table generated by this resource.
        Defaults to the decorated function name.
    - table_name: the name of the table, if different from the resource name.
    - write_disposition: controls how to write data to a table. Defaults to the value "append".
    '''
    yield pokemon_dict

########## CREATE A PIPELINE FROM A CSV ##########

# Define a resource to load data from a CSV
@dlt.resource(table_name='people_csv_data', write_disposition="replace")
def people_csv_resource():
  sample_df = pd.read_csv("https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv")
  yield sample_df

########## CREATE A PIPELINE FROM A SQL DATABASE ##########

@dlt.resource(table_name='genome_db_data', write_disposition="replace")
def genome_db_resource():
  engine = create_engine("mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam")
  with engine.connect() as conn:
      query = "SELECT * FROM genome LIMIT 1000"
      rows = conn.execution_options(yield_per=100).exec_driver_sql(query)
      yield from map(lambda row: dict(row._mapping), rows)

########## CREATE A PIPELINE FROM AN API ##########

# Define a resource to fetch pokemons from PokeAPI
@dlt.resource(table_name='pokemon_api_data', write_disposition="replace")
def pokemon_api_resource():
    url = "https://pokeapi.co/api/v2/pokemon"
    response = requests.get(url)
    yield response.json()["results"]

########## LOAD ALL RESOURCES INTO A SINGLE PIPELINE ##########

load_info = resource_pipeline.run([pokemon_dict_resource, people_csv_resource, genome_db_resource, pokemon_api_resource])
print(load_info)

# List all table names from the database
with resource_pipeline.sql_client() as client:
    with client.execute_query("SELECT table_name FROM information_schema.tables") as table:
        print("\n")
        print(f"All tables loaded in {resource_pipeline.pipeline_name}")
        print(table.df())

# Query the loaded data from 'pokemon_dict_data'
print("\n")
print("Pipeline from a dictionary")
dataset = resource_pipeline.dataset(dataset_type="default").pokemon_dict_data.df()
print(dataset)

# Query the loaded data from 'people_csv_data'
print("\n")
print("Pipeline from a CSV")
dataset = resource_pipeline.dataset(dataset_type="default").people_csv_data.df()
print(dataset)

# Query the loaded data from 'genome_db_data'
print("\n")
print("Pipeline from a SQL DATABASE")
dataset = resource_pipeline.dataset(dataset_type="default").genome_db_data.df()
print(dataset)

# Query the loaded data from 'pokemon_api'
print("\n")
print("Pipeline from an API")
dataset = resource_pipeline.dataset(dataset_type="default").pokemon_api_data.df()
print(dataset)

########## SOURCES ##########

# Create a source with all resources
@dlt.source
def all_data_source():
   return pokemon_dict_resource, people_csv_resource, genome_db_resource, pokemon_api_resource

# Create a new pipeline
source_pipeline = dlt.pipeline(
   pipeline_name="source_pipeline",
   destination="duckdb",
   dataset_name="source_schema"
)

# Load using the source
load_info = source_pipeline.run(all_data_source())
print(load_info)

# List all table names from the database
with source_pipeline.sql_client() as client:
    with client.execute_query("SELECT table_name FROM information_schema.tables") as table:
        print("\n")
        print(f"All tables loaded in {source_pipeline.pipeline_name}")
        print(table.df())

# Query the loaded data from 'pokemon_dict_data'
print("\n")
print(f"Pipeline from a dictionary - {source_pipeline.pipeline_name}")
dataset = source_pipeline.dataset(dataset_type="default").pokemon_dict_data.df()
print(dataset)

# Query the loaded data from 'people_csv_data'
print("\n")
print(f"Pipeline from a CSV - {source_pipeline.pipeline_name}")
dataset = source_pipeline.dataset(dataset_type="default").people_csv_data.df()
print(dataset)

# Query the loaded data from 'genome_db_data'
print("\n")
print(f"Pipeline from a SQL DATABASE - {source_pipeline.pipeline_name}")
dataset = source_pipeline.dataset(dataset_type="default").genome_db_data.df()
print(dataset)

# Query the loaded data from 'pokemon_api'
print("\n")
print(f"Pipeline from an API - {source_pipeline.pipeline_name}")
dataset = source_pipeline.dataset(dataset_type="default").pokemon_api_data.df()
print(dataset)

########## TRANSFORMERS ##########

# Define a transformer to enrich pokemon data with additional details
@dlt.transformer(data_from=pokemon_dict_resource, table_name='pokemon_detailed_info', write_disposition="replace")
def poke_details_transformer(items): # <--- `items` is a variable and contains data from `pokemon_dict` resource
    for item in items:

        print(f"Item: {item}\n") # <-- print what data we get from `pokemon_dict` source

        id = item["id"]
        url = f"https://pokeapi.co/api/v2/pokemon/{id}"
        response = requests.get(url)
        details = response.json()

        # You can optionally print what you get from the API. You'll see A TON of nested data.
        # print(f"Details: {details}\n") # <--- print what data we get from API

        yield details

# Create a new pipeline
transformer_pipeline = dlt.pipeline(
   pipeline_name="pokemon_transformer_pipeline",
   destination="duckdb",
   dataset_name="pokemon_details"
)

load_info = transformer_pipeline.run(poke_details_transformer())
print(load_info)



# List all table names from the database
with transformer_pipeline.sql_client() as client:
    table_names = []

    with client.execute_query("SELECT table_name FROM information_schema.tables") as table:
        print("\n")
        print(f"All tables loaded in {transformer_pipeline.pipeline_name}")

        tables_df = table.df()
        print(tables_df)

        table_names = []
        if tables_df is not None and not tables_df.empty:
            for _, row in tables_df.iterrows():
                table_names.append(row["table_name"])
            print("Tables found:", table_names)
        else:
            print("⚠️ No tables found or query failed.")

# Loop through each table name and fetch the data
for table in table_names:
    print("\n")
    print(f"Pipeline from a transformer - {transformer_pipeline.pipeline_name} - {table}")

    # Dynamically access the table

    dataset = getattr(transformer_pipeline.dataset(dataset_type="default"), table).df()

    #dataset = transformer_pipeline.dataset(dataset_type="default").table.df()

    print(dataset)